import streamlit as st
import numpy as np
import pandas as pd
import time
import plotly.graph_objects as go
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor
from multiprocessing import cpu_count
from functools import partial

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="Misdiagnosis Detection Tool",
    page_icon="ğŸ¥",
    layout='wide',
    initial_sidebar_state='expanded'
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
    <style>
    .metric-card {
        background: #f8f9fa;
        border-radius: 10px;
        padding: 20px;
        margin: 10px 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .risk-badge {
        padding: 4px 12px;
        border-radius: 15px;
        font-weight: 500;
    }
    .high-risk { background: #ffd7d5; color: #d32f2f; }
    .medium-risk { background: #fff4e5; color: #f57c00; }
    .low-risk { background: #e8f5e9; color: #388e3c; }
    </style>
""", unsafe_allow_html=True)

@st.cache_data
def load_and_preprocess(uploaded_file):
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    start = time.time()

    # æ•°æ®åŠ è½½
    df = pd.read_csv(uploaded_file, header=None, skiprows=1)
    df = df.iloc[:5000]  # ç¤ºä¾‹æ•°æ®é™åˆ¶

    # æ•°æ®æ¸…æ´—
    df.fillna('Missing', inplace=True)

    # ç‰¹å¾å·¥ç¨‹
    numeric_cols = df.select_dtypes(include=np.number).columns
    df[numeric_cols] = (df[numeric_cols] - df[numeric_cols].mean()) / df[numeric_cols].std()

    # æ•°æ®ç¼–ç 
    categorical = df.select_dtypes(exclude=np.number)
    encoded = pd.get_dummies(categorical, prefix_sep='::')

    # åˆå¹¶æ•°æ®é›†
    processed = pd.concat([df[numeric_cols], encoded], axis=1)

    print(f"Data processed in {time.time()-start:.2f}s")
    return processed

def find_patterns_updated(data):
    """æ”¹è¿›åçš„æ¨¡å¼å‘ç°ç®—æ³•"""
    patterns = defaultdict(lambda: [0, set()])
    for i in range(len(data)):
        for j in range(i, len(data)):
            # ä¿®æ”¹ï¼šä½¿ç”¨åˆ—è¡¨è€Œä¸æ˜¯é›†åˆæ¥ä¿æŒç»´åº¦ä¸€è‡´æ€§
            common_features = []
            for idx, (val_i, val_j) in enumerate(zip(data[i], data[j])):
                if val_i == val_j and val_i != 0:  # å‡è®¾0ä¸ºæ— ç‰¹å¾å€¼
                    common_features.append(idx)
            
            if common_features:
                pattern = tuple(common_features)
                patterns[pattern][0] += len(common_features)**2
                patterns[pattern][1].update([i, j])
    return dict(patterns)

def find_pure_patterns(patterns, opposite_data):
    """å¯»æ‰¾çº¯å‡€æ¨¡å¼"""
    pure_patterns = {}
    for pattern, (score, cases) in patterns.items():
        pure = True
        for case in cases:
            if case < len(opposite_data):  # æ·»åŠ è¾¹ç•Œæ£€æŸ¥
                feature_values = [opposite_data[case][i] if i < len(opposite_data[case]) else 0 for i in pattern]
                if any(v != 0 for v in feature_values):
                    pure = False
                    break
        if pure:
            pure_patterns[pattern] = (score, cases)
    return pure_patterns

def find_specific_instances(C, patterns_A, patterns_B, pure_A, pure_B):
    """è¯†åˆ«ç‰¹å®šå®ä¾‹ï¼Œä¿®æ”¹åçš„ç‰ˆæœ¬"""
    results = []
    for idx, c in enumerate(C):
        # ä¿®æ”¹ï¼šç¡®ä¿ç»´åº¦åŒ¹é…
        def calculate_pattern_score(pattern, instance):
            if not pattern:  # å¦‚æœæ¨¡å¼ä¸ºç©º
                return 0
            matches = sum(1 for i in pattern if i < len(instance) and instance[i] != 0)
            return matches**2

        # è®¡ç®—å¸¸è§„æ¨¡å¼åˆ†æ•°
        score_A = max((calculate_pattern_score(p, c) for p in patterns_A.keys()), default=0)
        score_B = max((calculate_pattern_score(p, c) for p in patterns_B.keys()), default=0)
        
        # è®¡ç®—çº¯å‡€æ¨¡å¼åˆ†æ•°
        pure_score_A = max((calculate_pattern_score(p, c) for p in pure_A.keys()), default=0)
        pure_score_B = max((calculate_pattern_score(p, c) for p in pure_B.keys()), default=0)
        
        results.append((c, score_A, score_B, pure_score_A, pure_score_B))
    
    return sorted(results, key=lambda x: x[3]+x[4], reverse=True)

def parallel_score_calc(data_chunk, ref_patterns):
    """å¹¶è¡Œè¯„åˆ†è®¡ç®—"""
    return [len(set(item) & ref_patterns) ** 2 for item in data_chunk]

class DiagnosisAnalyzer:
    def __init__(self, data):
        self.data = data
        self.patterns_cache = {}
        self.pure_patterns_cache = {}
        self.feature_dim = data.shape[1] if len(data.shape) > 1 else 1

    @st.cache_data
    def find_patterns(_self, class_type):
        """å¸¦ç¼“å­˜çš„æ¨¡å¼å‘ç°"""
        return find_patterns_updated(_self.data)

    def get_risk_level(self, score):
        """åŠ¨æ€é£é™©è¯„ä¼°"""
        if score > 3000: return 'Very High', '#ff4444'
        if score > 2000: return 'High', '#ffa500'
        if score > 1000: return 'Low', '#32cd32'
        return 'Very Low', '#808080'

    def get_advanced_patterns(self):
        """è·å–é«˜çº§æ¨¡å¼åˆ†æç»“æœ"""
        if not self.patterns_cache:
            mid_point = len(self.data)//2
            data_A = self.data[:mid_point]
            data_B = self.data[mid_point:]
            
            if len(data_A.shape) != 2 or len(data_B.shape) != 2:
                raise ValueError("æ•°æ®éœ€è¦æ˜¯2ç»´æ•°ç»„ï¼Œå½¢å¦‚ (æ ·æœ¬æ•°, ç‰¹å¾æ•°)")
            
            if data_A.shape[1] != data_B.shape[1]:
                raise ValueError(f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: A={data_A.shape[1]}, B={data_B.shape[1]}")
            
            self.patterns_cache['A'] = find_patterns_updated(data_A)
            self.patterns_cache['B'] = find_patterns_updated(data_B)
            
            self.pure_patterns_cache['A'] = find_pure_patterns(
                self.patterns_cache['A'], 
                data_B
            )
            self.pure_patterns_cache['B'] = find_pure_patterns(
                self.patterns_cache['B'],
                data_A
            )
        return self.patterns_cache, self.pure_patterns_cache

def render_sankey(analysis_data):
    """åŸºç¡€Sankeyå›¾ç”Ÿæˆ"""
    nodes = ['è¾“å…¥ç‰¹å¾', 'é˜³æ€§æ¨¡å¼', 'é˜´æ€§æ¨¡å¼']
    links = {
        'source': [0, 0],
        'target': [1, 2],
        'value': [analysis_data['pos_score'], analysis_data['neg_score']]
    }

    fig = go.Figure(go.Sankey(
        node=dict(
            pad=15,
            thickness=20,
            label=nodes,
            color=['#4a90e2', '#50c878', '#ff7373']
        ),
        link=dict(
            source=links['source'],
            target=links['target'],
            value=links['value']
        )
    ))

    fig.update_layout(
        title='è¯Šæ–­æ¨¡å¼æµå‘åˆ†æ',
        font=dict(size=14),
        height=500
    )
    return fig

def render_advanced_visualization(analysis_data, selected_index):
    """é«˜çº§Sankeyå›¾ç”Ÿæˆ"""
    c, score_A, score_B, pure_score_A, pure_score_B = analysis_data[selected_index]
    
    # è°ƒæ•´æ•°æ®ç»“æ„ä»¥é€‚åº”æ–°çš„æ¨¡å¼åŒ¹é…é€»è¾‘
    score_A_value = score_A if isinstance(score_A, (int, float)) else score_A[0]
    score_B_value = score_B if isinstance(score_B, (int, float)) else score_B[0]
    
    # ä¸»Sankeyå›¾
    fig = go.Figure(go.Sankey(
        node=dict(
            pad=15,
            thickness=20,
            label=[f'PATIENT:{selected_index+1}', 'Positive P', 'Negative N'],
            color=['#ECEFF1', '#F8BBD0', '#DCEDC8']
        ),
        link=dict(
            source=[0, 0],
            target=[1, 2],
            value=[score_A_value, score_B_value]
        )
    ))
    
    # Pure Sankeyå›¾
    pure_fig = go.Figure(go.Sankey(
        node=dict(
            pad=15,
            thickness=20,
            label=[f'PATIENT:{selected_index+1}', 'Pure P', 'Pure N'],
            color=['#ECEFF1', '#F8BBD0', '#DCEDC8']
        ),
        link=dict(
            source=[0, 0],
            target=[1, 2],
            value=[pure_score_A, pure_score_B]
        )
    ))
    
    return fig, pure_fig

def highlight_risk(row):
    """é£é™©é«˜äº®é€»è¾‘"""
    risk_level = row['ç»¼åˆé£é™©']
    color_map = {
        'Very High': '#ff4444',
        'High': '#ffa500',
        'Low': '#32cd32',
        'Very Low': '#808080'
    }
    return [f'background-color: {color_map[risk_level]}' for _ in row]

def main_interface():
    st.title('Misdiagnosis Detection Tool')
    st.markdown("---")

    with st.expander("ğŸ“ Upload Files", expanded=True):
        uploaded_file = st.file_uploader("Upload Filesï¼ˆCSVï¼‰", type="csv")

    if uploaded_file:
        try:
            data = load_and_preprocess(uploaded_file)
            
            if len(data.shape) != 2:
                st.error("ä¸Šä¼ çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®ã€‚è¯·ç¡®ä¿æ•°æ®æ˜¯2ç»´è¡¨æ ¼å½¢å¼ã€‚")
                return
                
            st.info(f"æ•°æ®ç»´åº¦: {data.shape[0]} æ ·æœ¬, {data.shape[1]} ç‰¹å¾")
            
            analyzer = DiagnosisAnalyzer(data.values)
            
            # å­˜å‚¨å¤„ç†åçš„æ•°æ®åˆ°session state
            st.session_state.processed_data = {
                'A': data[:len(data)//2].values,
                'B': data[len(data)//2:].values,
                'C': data.sample(n=min(20, len(data)), replace=False).values
            }

            # å®æ—¶åˆ†æé¢æ¿
            st.markdown("å®æ—¶åˆ†æé¢æ¿")
            col1, col2, col3 = st.columns(3)

            with col1:
                with st.container():
                    st.markdown("ğŸ§ª æ£€æµ‹æ ·æœ¬æ•°")
                    st.markdown(f'<div class="metric-card">{len(data):,}</div>', unsafe_allow_html=True)

            with col2:
                with st.container():
                    st.markdown("âš ï¸ é£é™©æç¤º")
                    risk_sample = data.sample(1).iloc[0]
                    st.markdown(f'''
                        <div class="metric-card">
                            <div>æœ€è¿‘è¯†åˆ«ç—…ä¾‹ï¼š</div>
                            <div class="risk-badge high-risk">é«˜å±</div>
                        </div>
                    ''', unsafe_allow_html=True)

            # æ‰©å±•æ ‡ç­¾é¡µ
            tab_analysis, tab_visual, tab_adv_visual, tab_report = st.tabs([
                "ğŸ“Š åŸºç¡€åˆ†æ", "ğŸ“ˆ æ¨¡å¼æµå‘", "ğŸ” é«˜çº§å¯è§†åŒ–", "ğŸ“ é£é™©æŠ¥å‘Š"
            ])

            with tab_analysis:
                with st.spinner('æ­£åœ¨åˆ†ææ•°æ®...'):
                    pos_patterns = analyzer.find_patterns('positive')
                    neg_patterns = analyzer.find_patterns('negative')

                st.dataframe(
                    pd.DataFrame.from_dict(pos_patterns, orient='index', columns=['å¼ºåº¦', 'å…³è”ç—…ä¾‹']),
                    height=400,
                    use_container_width=True
                )

            with tab_visual:
                sample_data = data.sample(1).iloc[0].values
                analysis_result = {
                    'pos_score': len(sample_data) * 150,
                    'neg_score': len(sample_data) * 75
                }
                st.plotly_chart(render_sankey(analysis_result), use_container_width=True)

            with tab_adv_visual:
                if 'processed_data' in st.session_state:
                    patterns_A, patterns_B = analyzer.get_advanced_patterns()[0]
                    pure_A, pure_B = analyzer.get_advanced_patterns()[1]
                    
                    specific_instances = find_specific_instances(
                        st.session_state.processed_data['C'],
                        patterns_A,
                        patterns_B,
                        pure_A,
                        pure_B
                    )
                    
                    total_instances = len(specific_instances)
                    choices = [f"ç—…ä¾‹ {i+1}" for i in range(total_instances)]
                    selected = st.selectbox("é€‰æ‹©åˆ†æç—…ä¾‹", options=choices)
                    
                    if selected:
                        index = int(selected.split()[-1]) - 1
                        fig, pure_fig = render_advanced_visualization(specific_instances, index)
                        
                        st.subheader("è¯Šæ–­æ¨¡å¼æµå‘")
                        st.plotly_chart(fig, use_container_width=True)
                        
                        st.subheader("çº¯å‡€æ¨¡å¼åˆ†æ")
                        st.plotly_chart(pure_fig, use_container_width=True)

            with tab_report:
                if 'processed_data' in st.session_state:
                    risk_data = []
                    for idx, (c, _, _, pure_A, pure_B) in enumerate(specific_instances):
                        risk_score = max(pure_A, pure_B)
                        risk_level, _ = analyzer.get_risk_level(risk_score)
                        
                        risk_data.append({ "ID": idx+1,
                        "é˜³æ€§åˆ†æ•°": pure_A,
                        "é˜´æ€§åˆ†æ•°": pure_B,
                        "ç»¼åˆé£é™©": risk_level,
                        "ç´§æ€¥ç¨‹åº¦": "âš ï¸" if risk_score > 2000 else ""
                    })
                
                df_risk = pd.DataFrame(risk_data)
                st.dataframe(
                    df_risk.style.apply(highlight_risk, subset=['ç»¼åˆé£é™©']),
                    height=600,
                    use_container_width=True
                )

                # æ·»åŠ é£é™©ç»Ÿè®¡æ‘˜è¦
                st.markdown("### é£é™©ç»Ÿè®¡æ‘˜è¦")
                risk_summary = pd.DataFrame(df_risk['ç»¼åˆé£é™©'].value_counts()).reset_index()
                risk_summary.columns = ['é£é™©ç­‰çº§', 'ç—…ä¾‹æ•°é‡']
                
                # ä½¿ç”¨plotlyåˆ›å»ºé£é™©åˆ†å¸ƒå›¾
                fig = go.Figure(data=[
                    go.Bar(
                        x=risk_summary['é£é™©ç­‰çº§'],
                        y=risk_summary['ç—…ä¾‹æ•°é‡'],
                        marker_color=['#ff4444', '#ffa500', '#32cd32', '#808080']
                    )
                ])
                
                fig.update_layout(
                    title='é£é™©ç­‰çº§åˆ†å¸ƒ',
                    xaxis_title='é£é™©ç­‰çº§',
                    yaxis_title='ç—…ä¾‹æ•°é‡',
                    height=400
                )
                
                st.plotly_chart(fig, use_container_width=True)

                # æ·»åŠ ç´§æ€¥å¤„ç†å»ºè®®
                st.markdown("### ç´§æ€¥å¤„ç†å»ºè®®")
                urgent_cases = df_risk[df_risk['ç´§æ€¥ç¨‹åº¦'] == "âš ï¸"]
                if not urgent_cases.empty:
                    st.warning(f"å‘ç° {len(urgent_cases)} ä¸ªéœ€è¦ç´§æ€¥å¤„ç†çš„ç—…ä¾‹")
                    st.dataframe(urgent_cases, use_container_width=True)
                else:
                    st.success("å½“å‰æ²¡æœ‰éœ€è¦ç´§æ€¥å¤„ç†çš„ç—…ä¾‹")

        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
            st.warning("è¯·æ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€…åˆ·æ–°é¡µé¢é‡è¯•ã€‚")

def run_analysis():
    """è¿è¡Œä¸»åˆ†ææµç¨‹"""
    try:
        main_interface()
    except Exception as e:
        st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
        st.warning("è¯·æ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€…åˆ·æ–°é¡µé¢é‡è¯•ã€‚")

if __name__ == "__main__":
    run_analysis()
